---
title: Undersampling and oversampling with tidymodels
author: Jose M Sallan
date: '2022-10-10'
slug: undersampling-and-oversampling-with-tidymodels
categories:
  - R
tags:
  - decision trees
  - machine learning
  - tidymodels
meta_img: images/image.png
description: Description for the page
---



<p>In this post, I will present how oversampling and undersampling can help us in a classification job on an unbalanced dataset. In unbalanced datasets, the target variable has an uneven distribution, with salient majority and minority classes. Undersampling and oversampling try to improve model performance training the model with a balanced dataset. When using <strong>undersampling</strong>, we train the model with a set removing observations of the majority class. With <strong>oversampling</strong>, we train the model with a dataset with additional artificial elements of the minority class.</p>
<p>I will be using <code>tidymodels</code> for the prediction workflow, <code>BAdatasets</code> to access the dataset and <code>themis</code> to perform undersampling and oversampling.</p>
<pre class="r"><code>library(tidymodels)
library(themis)
library(BAdatasets)</code></pre>
<p>I will be using the <code>cc_defaults</code> dataset. It is a dataset of defaults payments in Taiwan, presented in Yeh &amp; Lien (2009).</p>
<pre class="r"><code>data(&quot;cc_defaults&quot;)</code></pre>
<p>When plotting the proportion of observations for each case, we observe that the number of negative cases (not defaulted) is much larger than the positive cases (defaulted). Therefore, the set of positive cases is the minority class. This is a frequent situation in contexts like loan defaults or credit card fraud.</p>
<pre class="r"><code>cc_defaults |&gt;
  ggplot(aes(factor(default))) + 
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count)))) +
  scale_y_continuous(labels=percent) +
  labs(x = &quot;default&quot;, y = &quot;% of cases&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="workflow-elements" class="section level2">
<h2>Workflow elements</h2>
<p>Let’s define the elements of the <code>tidymodels</code> workflow. We start transforming the target <code>default</code> variable into a factor, setting the level of positive cases first.</p>
<pre class="r"><code>cc_defaults &lt;- cc_defaults |&gt;
  mutate(default = factor(default, levels = c(&quot;yes&quot;, &quot;no&quot;)))</code></pre>
<p>Then, we need to split the data into train and test. As the dataset is large, I have selected a large <code>prop</code> value to include in the train set. Being the dataset unbalanced, it is important to set <code>strata = "default"</code>, so that the distribution of target variable in train and test sets will be similar to the original dataset. I have applied a similar logic to split the training set into five <code>folds</code> to apply cross validation.</p>
<pre class="r"><code>set.seed(1313)
split &lt;- initial_split(cc_defaults, prop = 0.9, strata = &quot;default&quot;)

folds &lt;- vfold_cv(training(split), v = 5, strata = &quot;default&quot;)</code></pre>
<p>To evaluate model performance, I have chosen the following metrics:</p>
<ul>
<li><code>accuracy</code>: fraction of observations correctly classified.</li>
<li>sensitivity <code>sens</code>: the fraction of positive elements correctly classified.</li>
<li>specificity <code>spec</code>: the fraction of negative elements correctly classified.</li>
<li>area under the ROC curve <code>roc_auc</code>: a parameter assessing the tradeoff between <code>sens</code> and <code>spec</code>.</li>
</ul>
<pre class="r"><code>class_metrics &lt;- metric_set(accuracy, sens, spec, roc_auc)</code></pre>
<p>In unbalanced datasets, accuracy can be a bad metric of performance. If 90% of observations belong to the negative class, we obtain an accuracy of 0.9 simply classifying all observations as negative. In that context, usually sensitivity is a more adequate metric. Additionally, in jobs like loan defaults or credit card fraud, the cost of a false negative is much higher than of a false positive.</p>
<p>I will use a decision tree <code>dt</code> model, setting standard parameters.</p>
<pre class="r"><code>dt &lt;- decision_tree(mode = &quot;classification&quot;, cost_complexity = 0.1, min_n = 10) |&gt;
  set_engine(&quot;rpart&quot;)</code></pre>
<p>The preprocessing recipe has the following steps:</p>
<ul>
<li>variables <code>id</code> and <code>pay_sep</code> to <code>pay_apr</code> are removed. Before that, I am replacing <code>pay_sep</code> with payment_status.</li>
<li>variables <code>payment_status</code>, <code>marriage</code> and <code>education</code> are transformed so that abnormal values are set to <code>NA</code>.</li>
<li><code>NA</code> values of predictors are imputed usign <em>k</em> nearest neighbors.</li>
</ul>
<pre class="r"><code>rec_base &lt;- training(split) |&gt;
  recipe(default ~ .) |&gt;
  step_rm(id) |&gt;
  step_mutate(payment_status = ifelse(pay_sep &lt; 1, 0, pay_sep)) |&gt;
  step_rm(pay_sep:pay_apr) |&gt;
  step_mutate(marriage = ifelse(!marriage %in% 1:3, NA, marriage)) |&gt;
  step_mutate(education = ifelse(!education %in% 1:4, NA, education)) |&gt;
  step_impute_knn(impute_with = all_predictors(), neighbors = 5)</code></pre>
<p>The steps to perform undersampling and oversampling are provided by <code>themis</code> package. Here I am using the following methods:</p>
<ul>
<li><code>step_downsample</code> performs random majority under-sampling with replacement.</li>
<li><code>step_upsample</code> performs random minority over-sampling with replacement.</li>
</ul>
<p>These steps are added to the <code>rec_base</code> recipe to obtain new recipes <code>rec_us</code> and <code>rec_os</code> for under and oversampling, respectively.</p>
<pre class="r"><code>rec_us &lt;- rec_base |&gt;
  step_downsample(default, under_ratio = 1)

rec_os &lt;- rec_base |&gt;
  step_upsample(default, over_ratio = 1)</code></pre>
</div>
<div id="testing-under--and-oversampling-with-cross-validation" class="section level2">
<h2>Testing under- and oversampling with cross validation</h2>
<p>Now we are ready to test each of the three models with cross validation. ´cv_unb<code>,</code>cv_us<code>and</code>cv_os` train the model with the original dataset, undersampling and oversampling respectively.</p>
<pre class="r"><code>cv_unb &lt;- fit_resamples(object = dt,
                        preprocessor = rec_base,
                        resamples = folds,
                        metrics = class_metrics)

cv_us &lt;- fit_resamples(object = dt,
                        preprocessor = rec_us,
                        resamples = folds,
                        metrics = class_metrics)

cv_os &lt;- fit_resamples(object = dt,
                        preprocessor = rec_os,
                        resamples = folds,
                        metrics = class_metrics)</code></pre>
<p>Now we can extract the metrics for each of the cross-validations. We can stack them all in a single data frame <code>m</code>.</p>
<pre class="r"><code>m_unb &lt;- collect_metrics(cv_unb) |&gt;
  mutate(train = &quot;unbalanced&quot;)

m_us &lt;- collect_metrics(cv_us) |&gt;
  mutate(train = &quot;undersampling&quot;)

m_os &lt;- collect_metrics(cv_os) |&gt;
  mutate(train = &quot;oversampling&quot;)

m &lt;- bind_rows(m_unb, m_us, m_os) |&gt;
  mutate(train = factor(train, levels = c(&quot;unbalanced&quot;, &quot;undersampling&quot;, &quot;oversampling&quot;)))</code></pre>
<p>Let’s see the results:</p>
<pre class="r"><code>ggplot(m, aes(.metric, mean, fill = train)) +
  geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) +
  theme_minimal() +
  labs(title = &quot;Performance of cross validation&quot;, x = &quot;metric&quot;, y = &quot;value&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>In this case, oversampling and undersampling obtain the same results. Both allow improving sensitivity significantly (although the obtained value is quite poor), paying the price of slightly worsening specificity. Global parameters like accuracy and AUC are not significantly affected.</p>
</div>
<div id="evaluating-the-model-in-the-test-set" class="section level2">
<h2>Evaluating the model in the test set</h2>
<p>Now I will fit the model with the whole train set and examine its performance in the test set. In <code>class_metrics2</code> I am excluding <code>roc_auc</code> because I will obtain the class predicted only.</p>
<pre class="r"><code>class_metrics2 &lt;- metric_set(accuracy, sens, spec)</code></pre>
<p>Objects <code>unb_model</code>, <code>us_model</code> and <code>os_model</code> contain the model trained in the original, undersampled and oversampled recipes respectively.</p>
<pre class="r"><code>unb_model &lt;- workflow() |&gt;
  add_recipe(rec_base) |&gt;
  add_model(dt) |&gt;
  fit(training(split))

us_model &lt;- workflow() |&gt;
  add_recipe(rec_us) |&gt;
  add_model(dt) |&gt;
  fit(training(split))

os_model &lt;- workflow() |&gt;
  add_recipe(rec_os) |&gt;
  add_model(dt) |&gt;
  fit(training(split))</code></pre>
<p>Next, I am storing in <code>df_test</code> the results of evaluating each of the models in the test set. Note that we evaluate the model in the original dataset. Under- and oversampling are only performed to train the model. The datasets where the model is evaluated are not modified.</p>
<pre class="r"><code>m_test &lt;- lapply(list(unb_model, us_model, os_model), function(x) x |&gt;
         predict(testing(split)) |&gt;
         bind_cols(testing(split)) |&gt;
         class_metrics2(truth = default, estimate = .pred_class))
df_test &lt;- bind_rows(m_test) |&gt;
  mutate(train = rep(c(&quot;unbalanced&quot;, &quot;undersampling&quot;, &quot;oversampling&quot;), each = 3)) |&gt;
  mutate(train = factor(train, levels = c(&quot;unbalanced&quot;, &quot;undersampling&quot;, &quot;oversampling&quot;)))</code></pre>
<p>Here are the results of the evaluation. They are quite similar to the obtained with cross validation.</p>
<pre class="r"><code>ggplot(df_test, aes(.metric, .estimate, fill = train)) +
  geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) +
  theme_minimal() +
  labs(title = &quot;Performance of test set&quot;, x = &quot;metric&quot;, y = &quot;value&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Under- and oversampling can be useful techniques to improve the ratio of true evaluations of the minority class in unbalanced datasets. So these techniques tend to increase sensitivity at the price of worse values of specificity. This can be a good compromise in classification jobs where the cost of a false positive is much higher than of a false negative.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><code>BAdatasets</code> package <a href="https://github.com/jmsallan/BAdatasets" class="uri">https://github.com/jmsallan/BAdatasets</a></li>
<li><code>themis</code> package website: <a href="https://themis.tidymodels.org/" class="uri">https://themis.tidymodels.org/</a></li>
<li>Yeh, I. C., &amp; Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. <em>Expert Systems with Applications</em>, 36(2), 2473-2480. <a href="https://doi.org/10.1016/j.eswa.2007.12.020" class="uri">https://doi.org/10.1016/j.eswa.2007.12.020</a></li>
</ul>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre><code>## R version 4.4.2 (2024-10-31)
## Platform: x86_64-pc-linux-gnu
## Running under: Linux Mint 21.1
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/blas/libblas.so.3.10.0 
## LAPACK: /usr/lib/x86_64-linux-gnu/lapack/liblapack.so.3.10.0
## 
## locale:
##  [1] LC_CTYPE=es_ES.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=es_ES.UTF-8    
##  [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=es_ES.UTF-8   
##  [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       
## 
## time zone: Europe/Madrid
## tzcode source: system (glibc)
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] rpart_4.1.23       BAdatasets_0.2.0   themis_1.0.2       yardstick_1.3.1   
##  [5] workflowsets_1.1.0 workflows_1.1.4    tune_1.2.1         tidyr_1.3.1       
##  [9] tibble_3.2.1       rsample_1.2.1      recipes_1.0.10     purrr_1.0.2       
## [13] parsnip_1.2.1      modeldata_1.3.0    infer_1.0.7        ggplot2_3.5.1     
## [17] dplyr_1.1.4        dials_1.2.1        scales_1.3.0       broom_1.0.5       
## [21] tidymodels_1.2.0  
## 
## loaded via a namespace (and not attached):
##  [1] tidyselect_1.2.1    timeDate_4032.109   farver_2.1.1       
##  [4] fastmap_1.1.1       blogdown_1.19       digest_0.6.35      
##  [7] timechange_0.3.0    lifecycle_1.0.4     ellipsis_0.3.2     
## [10] survival_3.8-3      magrittr_2.0.3      compiler_4.4.2     
## [13] rlang_1.1.3         sass_0.4.9          tools_4.4.2        
## [16] utf8_1.2.4          yaml_2.3.8          data.table_1.15.4  
## [19] knitr_1.46          labeling_0.4.3      DiceDesign_1.10    
## [22] withr_3.0.0         nnet_7.3-20         grid_4.4.2         
## [25] fansi_1.0.6         colorspace_2.1-0    future_1.33.2      
## [28] globals_0.16.3      iterators_1.0.14    MASS_7.3-64        
## [31] cli_3.6.2           rmarkdown_2.26      generics_0.1.3     
## [34] rstudioapi_0.16.0   future.apply_1.11.2 cachem_1.0.8       
## [37] splines_4.4.2       parallel_4.4.2      vctrs_0.6.5        
## [40] hardhat_1.3.1       Matrix_1.7-1        jsonlite_1.8.8     
## [43] bookdown_0.39       listenv_0.9.1       foreach_1.5.2      
## [46] gower_1.0.1         jquerylib_0.1.4     glue_1.7.0         
## [49] parallelly_1.37.1   codetools_0.2-19    lubridate_1.9.3    
## [52] gtable_0.3.5        munsell_0.5.1       GPfit_1.0-8        
## [55] ROSE_0.0-4          pillar_1.9.0        furrr_0.3.1        
## [58] htmltools_0.5.8.1   ipred_0.9-14        lava_1.8.0         
## [61] R6_2.5.1            lhs_1.2.0           evaluate_0.23      
## [64] lattice_0.22-5      highr_0.10          backports_1.4.1    
## [67] bslib_0.7.0         class_7.3-23        Rcpp_1.0.12        
## [70] prodlim_2023.08.28  xfun_0.43           pkgconfig_2.0.3</code></pre>
<p>Modified for small typos and dataset update on 2025-01-13.</p>
</div>
