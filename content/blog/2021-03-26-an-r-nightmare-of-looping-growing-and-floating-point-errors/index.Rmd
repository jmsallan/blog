---
title: An R nightmare of looping, growing and floating point errors
author: Jose M Sallan
date: '2021-03-26'
slug: an-R-nightmare-of-looping,-growing-and-floating-point-errors
categories:
  - R
tags:
  - optimization
  - R
meta_img: images/image.png
description: Description for the page
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let's do a simple task: generating the *n* first terms of a **geometric progression** of ratio $r$ and scale $a$. The elements of this progression can be defined for $n > 0$ as:

\[ a_n = ar^{n-1}  \]

or recursively as:

\[ a_{n} = ra_{n-1} \]

with $a_1 = a$.

Specifically, I am interested in the case $0 < r < 1$.

Let's see three ways of defining this geometric progression, that will allow us to examine the problems of:

* growing objects without allocating memory first,
* iterating instead of using vectorized expressions,
* not considering numerical error when comparing real numbers.

We were warned about those problems by Patrick Burns in his *The R Inferno book*. Here we can see how they emerge when doing a simple task, and how to control them.

## Three ways of building a geometric progression

Let's define three functions to generate the `n` first terms of a geometric progression of ratio `r` and scale `a`. We are tempted of looping because of the recursive nature of the progression:

```{r}
f1 <- function(n, a, r){
  result <- a
  for(i in 1:(n-1)) result <- c(result, r*result[i])
  return(result)
}
```

Once we are done, we think that maybe there is something wrong in making an object grow in each iteration. In this second function I allocate memory first, and then assign each element recursively:

```{r}
f2 <- function(n, a, r){
  result <- numeric(n)
  result[1] <- a
  for(i in 2:n) result[i] <- result[i-1]*r
  return(result)
}
```

But finally, I remember that R is a vectorial language, and that looping in a vectorial language can be a bad idea. So I use that $a_{n} = ra_{n-1}$ to define the sequence vectorially:

```{r}
f3 <- function(n, a, r) return(a * r^seq(0, n-1, 1))
```

Let's examine the results of each function with a small sequence:

```{r}
f1(50, 10, 0.9)
f2(50, 10, 0.9)
f3(50, 10, 0.9)
```

## Are the results equal?

Although results are apparently equal, it is a good thing to examine that they are *really* equal with `identical`:

```{r}
identical(f1(50, 10, 0.9), f2(50, 10, 0.9))
identical(f1(50, 10, 0.9), f3(50, 10, 0.9))
```

How can be that the results of `f1` and `f3` are different, if they look pretty much the same? Let's make the difference between the results of both, that should be exactly zero in each component:

```{r}
f3(50, 10, 0.9) - f1(50, 10, 0.9)
```

In some components, there are very small differences between the results. This is caused by **numerical error**, coming from representing real numbers with a limited amount of  space usign **floating point arithmetic**. Quoting Patrick Burn's *The R inferno*:

> Do not confuse numerical error with an error. An error is when a computation is wrongly performed. Numerical error is when there is visible noise resulting from the finite representation of numbers. It is numerical error—not an error—when one-third is represented as 33%.

A possible way of managing this is not to look for exact numerical equality, but allow a small **tolerance** instead:

```{r}
tol <- 1e-8
all(f3(50, 10, 0.9) - f1(50, 10, 0.9) < tol)
```

Take this issue into account whenever you have to compare real numbers, as the problem may arise also for inequalities.

## Benchmarking the three functions

Let's examine the performance of the three functions using `rbenchmark`:

```{r}
geom_benchmark <- rbenchmark::benchmark(f1(1000, 10, 0.9), f2(1000, 10, 0.9), f3(100, 10, 0.9),
                      replications=100,
                      order = "relative",
                      columns = c("test", "replications", "elapsed", "relative"))
geom_benchmark
```

The most effective function is the vectorized `f3`. We also observe that:

* performance is `r geom_benchmark$relative[2]` worse when using `for` loops
* and is `r round(geom_benchmark$relative[3]/geom_benchmark$relative[2], 3)` times worse when growing the vector in each iteration instead of allocating memory at first (performance results vary in each system and each run).

Growing objects can be even worse than looping. Let's quote Patrick Burns again:

> You may wonder why growing objects is so slow. It is the computational equivalent of suburbanization. When a new size is required, there will not be enough room where the object is; so it needs to move to a more open space. (...) You end up with lots of small pieces of available memory, but no large pieces. This is called fragmenting memory.

## Recommendations

From this simple example, we have learned that:

* we must **avoid looping** when posible in vectorized operations,
* but also avoid **growing objects** without allocating memory,
* and compare real numbers using a **tolerance** value.

## Reference

Burns, Patrick (2011). The R Inferno. Avaiable at: <https://www.burns-stat.com/pages/Tutor/R_inferno.pdf>

