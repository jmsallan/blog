---
title: Hierarchical linear regression
author: Jose M Sallan
date: '2021-07-23'
slug: hierarchical-linear-regression
categories:
  - R
tags:
  - R
  - linear regression
meta_img: images/image.png
description: Description for the page
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(stargazer)
library(igraph)
```

**Hierarchical linear regression** is a way to examine if a set of **predictor** variables explains a **criterion** variable, after accounting the effect of **control variables**. In this modelling framework, we build several linear regression models adding blocks of variables at each step. Comparing the outcomes of the regression models, we can determine if the new block of variables increases significantly the proportion of explained variance of the criterion.

I will introduce hierarchical linear regression with three examples:

* a first dataset where there is a spurious correlation between predictor and criterion,
* a second dataset where there is a true relationship between criterion and predictor
* and a third dataset taken from University of Virginia Library, that clarifies the distinction between predictor and criterion.

I will be using the text output of the `stargazer` package to present the results of hierarchical linear regression models.

## First dataset

Let's build a set of artificial data to explain why we should be accounting for the effect of variables unrelated to the model:

```{r}
n <- 100
con <- sample(25:45, n, replace = TRUE)
data01 <- data.frame(cri = 4 + 2*con + rnorm(n, mean = 0, sd = 2),
                     pre = 3 + con + rnorm(n, mean = 0, sd = 4),
                     con = con)


mod01 <- lm(cri ~ pre, data01)
```

In the data01 dataset, the supposed predictor and criterion variables depend jointly on the control variable:

```{r, fig.align='center', fig.width=4, echo=FALSE}

g01 <- graph.data.frame(data.frame(org = c("con", "con"), dst = c("pre","cri")))
g01_layout <- matrix(c(0, 0, 1, -1, 1, 1), 3, 2, byrow = TRUE)

par(oma=c(0, 0, 0, 0), mar = c(0, 0, 0, 0))
plot(g01, layout = g01_layout, vertex.size = 40, vertex.shape = "square", vertex.color = "#FFFFFF", vertex.label.family = "Helvetica")
```

We cannot detect that if we examine the correlation matrix:

```{r}
cor(data01)
```

We observe a strong correlation between criterion and predictor. This is not desirable, because ideally regressors must be uncorrelated. If we ignore this warning and naively regress the criterion on the predictor, we obtain a significant relationship between both variables, which does not exist:

```{r}
mod01 <- lm(cri ~ pre, data = data01)
stargazer(mod01, type = "text")
```

To avoid this pitfall, we can adopt a hierarchical linear regression modelin framework with two models:

* a **first model** `mod01a` where we regress the criterion on the control variables
* and a **second model** `mod01b` where we regress the criterion on the control *and* predictor variables.

```{r}
mod01a <- lm(cri ~ con, data01)
mod01b <- lm(cri ~ con + pre, data01)
```

I use the stargazer package to present both models together:

```{r}
stargazer(mod01a, mod01b, type = "text")
```

Now we observe that the **coefficient of the predictor variable** is not signiificant when we put it together with the control variable. We also observe that the coefficient of the control variable does not change significantly when we introduce the predictor. We conclude that the criterion does not depend on the predictor.

If we observe the **R-square** of both models, we learn that the second model is not better than the first. In fact, the adjusted R-square of the second model is worse than the first. Additionnaly, we can test the null hypothesis that the second model is not better than the first using the `anova` function:

```{r}
anova(mod01b, mod01a)
```

The low *p*-value makes us think that there are no sound reasons to discard the null hypothesis in this case.

## Second dataset

Let's build a new dataset, where the criterion variable depends on the predictor and control variables:

```{r}
pre <- runif(n, 20, 100)
data02 <- data.frame(cri = 4 + 2*pre + 3*con + rnorm(n, mean = 0, sd = 10),
                     pre = pre,
                     con = con)
```

For this dataset, we know that the existing relationships are:

```{r, fig.align='center', fig.width=4, echo=FALSE}

g02 <- graph.data.frame(data.frame(org = c("pre", "con"), dst = c("cri","cri")))
g02_layout <- matrix(c(-1, 1, -1, -1, 0, 0), 3, 2, byrow = TRUE)

par(oma=c(0, 0, 0, 0), mar = c(0, 0, 0, 0))
plot(g02, layout = g02_layout, vertex.size = 40, vertex.shape = "square", vertex.color = "#FFFFFF", vertex.label.family = "Helvetica")
```

Now we observe a lower correlation between predictor and control, and a strong correlation between criterion and predictor:

```{r}
cor(data02)
```

Let's build the two models for the hierarchical linear regression, and present the obtained results:

```{r}
mod02a <- lm(cri ~ con, data02)
mod02b <- lm(cri ~ con + pre, data02)
stargazer(mod02a, mod02b, type = "text")
```

Now the **coefficient of the predictor** variable is significant, when put together with the control variable. We also observe that the coefficient of the control variable keeps being significant in the second model. We observe a significant increase of the **R-square** of the second model respect to the first. Finnally, we also observe that the anova test is significant:

```{r}
anova(mod02b, mod02a)
```

Now we can conclude that there is a significant relationship between predictor and criterion, after controlling by control variables.

## Third dataset

Let's perform now hierarchical regression on a dataset with five variables, obtained from the University of Virginia Library:

```{r}
happiness_data <- read.csv('http://static.lib.virginia.edu/statlab/materials/data/hierarchicalRegressionData.csv')
head(happiness_data)
```

Our aim is to determine if happiness increases when we have more friends and more pets, controlling by age and gender. By controlling by this demographic variables, we rid out the possibility that happinness and number of friends or pets depend on age or gender.

Let's examine first the correlation matrix between numerical variables. We observe a low correlation between controls and predictors:

```{r}
cor(happiness_data[, c(1:2, 4:5)])
```

And let's examine the result of the hierarchical linear regression model:

```{r}
hmod01 <- lm(happiness ~ age + gender, data=happiness_data)
hmod02 <- lm(happiness ~ age + gender + friends + pets, data=happiness_data)
stargazer(hmod01, hmod02, type = "text")
```

Here we observe that:

* The status of the coefficients of the two control variables does not change when we add the predictor variables
* The coefficients of the two predictor variables are significant and positive.

Additionally, the ANOVA between the two models is significant:

```{r}
anova(hmod02, hmod01)
```

Then, from that analysis we conclude that happiness is positively related with number of friends and number of pets, after controlling by gender and age.

## References

* Hierarchical Linear Regression (University of Virginia Library) <https://data.library.virginia.edu/hierarchical-linear-regression/>
* `stargazer` cheatsheet <https://www.jakeruss.com/cheatsheets/stargazer/>

*Built with R 4.1.0, stargazer 5.2.2 and igraph 1.2.6*