---
title: A workflow for numerical prediction with tidymodels
author: Jose M Sallan
date: '2021-05-07'
slug: a-workflow-for-numerical-prediction-with-tidymodels
categories:
  - R
tags:
  - machine learning
  - tidymodels
meta_img: images/image.png
description: Description for the page
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p><code>tidymodels</code> is a collection of packages for modelling and machine learning in R, drawing on the tools and approach of the <code>tidyverse</code>. In <a href="https://jmsallan.netlify.app/blog/a-workflow-for-binary-classification-with-tidymodels/">a recent post</a> I introduced the basic flow of <code>tidymodels</code> with a small classification example. In this post, I will present some additional features like:</p>
<ul>
<li>how to perform a <strong>regression</strong> or <strong>numerical prediction</strong> job,</li>
<li>how to split data into <strong>train and test</strong> sets,</li>
<li>how to assess the performance of competing models on the training set with <strong>cross validation</strong>.</li>
</ul>
<p>We will use the <code>BostonHousing</code> dataset, available from the <code>mlbench</code> package. The rest of functionalities used here come from <code>tidymodels</code>, although you may be requested to install <code>rpart</code> if you want to reproduce this code.</p>
<pre class="r"><code>library(tidymodels)
library(mlbench)
data(&quot;BostonHousing&quot;)
BostonHousing %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 506
## Columns: 14
## $ crim    &lt;dbl&gt; 0.00632, 0.02731, 0.02729, 0.03237, 0.06905, 0.02985, 0.08829,…
## $ zn      &lt;dbl&gt; 18.0, 0.0, 0.0, 0.0, 0.0, 0.0, 12.5, 12.5, 12.5, 12.5, 12.5, 1…
## $ indus   &lt;dbl&gt; 2.31, 7.07, 7.07, 2.18, 2.18, 2.18, 7.87, 7.87, 7.87, 7.87, 7.…
## $ chas    &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ nox     &lt;dbl&gt; 0.538, 0.469, 0.469, 0.458, 0.458, 0.458, 0.524, 0.524, 0.524,…
## $ rm      &lt;dbl&gt; 6.575, 6.421, 7.185, 6.998, 7.147, 6.430, 6.012, 6.172, 5.631,…
## $ age     &lt;dbl&gt; 65.2, 78.9, 61.1, 45.8, 54.2, 58.7, 66.6, 96.1, 100.0, 85.9, 9…
## $ dis     &lt;dbl&gt; 4.0900, 4.9671, 4.9671, 6.0622, 6.0622, 6.0622, 5.5605, 5.9505…
## $ rad     &lt;dbl&gt; 1, 2, 2, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,…
## $ tax     &lt;dbl&gt; 296, 242, 242, 222, 222, 222, 311, 311, 311, 311, 311, 311, 31…
## $ ptratio &lt;dbl&gt; 15.3, 17.8, 17.8, 18.7, 18.7, 18.7, 15.2, 15.2, 15.2, 15.2, 15…
## $ b       &lt;dbl&gt; 396.90, 396.90, 392.83, 394.63, 396.90, 394.12, 395.60, 396.90…
## $ lstat   &lt;dbl&gt; 4.98, 9.14, 4.03, 2.94, 5.33, 5.21, 12.43, 19.15, 29.93, 17.10…
## $ medv    &lt;dbl&gt; 24.0, 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15…</code></pre>
<p>Our job will be predicting the median value of owner-occupied homes in USD <code>medv</code> of each of the 508 Boston census tracts. This is a continous variable, so it is a numerical prediction or regression problem.</p>
<div id="creating-train-and-test-sets" class="section level2">
<h2>Creating train and test sets</h2>
<p>The aim of a predictive model is to perform well on unseen data, not used to train the model. To assess model perfomance, we need to split our dataset into two subsets:</p>
<ul>
<li>a <strong>train set</strong> of observations used to obtain (train) the model,</li>
<li>a <strong>test set</strong> of observations that will be used to assess model performance.</li>
</ul>
<p>Regarding train and test sets, we must take care that:</p>
<ul>
<li>the train and test sets must be representative of the same population,</li>
<li>the test set must not be used to make any decision regarding the model, it must be used to assess model performance only.</li>
</ul>
<p>We use the <code>initial_split</code> function to build the train and test sets:</p>
<ul>
<li>with <code>p = 0.7</code> we set the 70% of observations in the train set,</li>
<li>with <code>strata = "medv"</code> we perform stratified sampling so that the distribution of the target variable is similar to the whole sample in both sets.</li>
</ul>
<p>As data splitting implies randomness, I have fixed the seed of the pseudo-random number generator to ensure reproducibility.</p>
<pre class="r"><code>set.seed(1313)
bh_split &lt;- initial_split(BostonHousing, p = 0.7, strata = &quot;medv&quot;)</code></pre>
</div>
<div id="data-preprocessing" class="section level2">
<h2>Data preprocessing</h2>
<p>The recipe to pre process the dataset includes:</p>
<ul>
<li>the transformation of <code>chas</code> from factor to numeric with <code>step_dummy</code>. As the original chas value has only two levels, this steps generates a single dummy variable.</li>
<li>looking for correlated predictors with <code>step_corr</code>, and for low-variability predictors with <code>step_nzv</code>.</li>
</ul>
<pre class="r"><code>bh_recipe &lt;- training(bh_split) %&gt;%
  recipe(medv ~ .) %&gt;%
  step_dummy(chas) %&gt;%
  step_corr(all_predictors()) %&gt;%
  step_nzv(all_predictors()) %&gt;%
  prep()</code></pre>
<p>As we have prepared the recipe with <code>prep()</code> we can see how it is working:</p>
<pre class="r"><code>bh_recipe</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         13
## 
## Training data contained 356 data points and no missing data.
## 
## Operations:
## 
## Dummy variables from chas [trained]
## Correlation filter removed tax [trained]
## Sparse, unbalanced variable filter removed no terms [trained]</code></pre>
<p>The recipe is removing the <code>tax</code> variable, highly correlated with other variables:</p>
</div>
<div id="defining-models-and-workflows" class="section level2">
<h2>Defining models and workflows</h2>
<p>We will train two models with this dataset:</p>
<ul>
<li>a <strong>linear regression</strong> model, using the R base <code>lm</code> function,</li>
<li>a <a href="https://uc-r.github.io/regression_trees"><strong>regression tree</strong></a> model, using the <code>rpart</code> package.</li>
</ul>
<p>We will also define a workflow for each model so that we do the pre-processing with <code>bh_recipe</code>.</p>
<pre class="r"><code>bh_lm &lt;- linear_reg(mode = &quot;regression&quot;) %&gt;%
  set_engine(&quot;lm&quot;)

bh_lm_wf &lt;- workflow() %&gt;%
  add_recipe(bh_recipe) %&gt;%
  add_model(bh_lm)

bh_rt &lt;- decision_tree(mode = &quot;regression&quot;) %&gt;%
  set_engine(&quot;rpart&quot;)

bh_rt_wf &lt;- workflow() %&gt;%
  add_recipe(bh_recipe) %&gt;%
  add_model(bh_rt)</code></pre>
</div>
<div id="comparing-models-with-cross-validation" class="section level2">
<h2>Comparing models with cross validation</h2>
<p>We need to compare the performance of regression tree and linear regression models, preferably with a dataset different to train the model to avoid overfitting. As we cannot make any decision about the model using the test set, we can use a cross validation strategy:</p>
<ul>
<li>randomly split the data into <em>v</em> folds <em>f</em> of approximately equal size,</li>
<li>for each fold <em>f</em>, we train the data with the other <em>f-1</em> folds and assess performance on <em>f</em>,</li>
<li>optionally we repeat this <code>repeats</code> times,</li>
<li>we average model performance accross all folds and repeats.</li>
</ul>
<p>We use the <code>vfold_cv</code> function to build the cross validation framework. I am defining three folds and three repeats, so we will perform nine evaluations of each model. With larger datasets, we can split the train set into more folders. We stratify by the dependent variable like in <code>initial_split</code>, so I am setting up againt the random number generator:</p>
<pre class="r"><code>set.seed(1212)
bh_folds &lt;- vfold_cv(training(bh_split), strata = &quot;medv&quot;, v = 3, repeats = 3)</code></pre>
</div>
<div id="model-performance-metrics" class="section level2">
<h2>Model performance metrics</h2>
<p>We will use three metrics to assess model performance. These metrics examine how close are predictions <span class="math inline">\(\hat{y}_i\)</span> from observations <span class="math inline">\(y_i\)</span> across all <span class="math inline">\(n\)</span> observations.</p>
<p>The <strong>root mean square error</strong> <code>rmse</code>:</p>
<p><span class="math display">\[ \sqrt{\frac{\sum \left( \hat{y}_i - y_i \right)^2}{n}} \]</span></p>
<p>The <strong>mean absolute error</strong> <code>mae</code>:</p>
<p><span class="math display">\[  \frac{\sum \vert \hat{y}_i - y_i \vert}{n} \]</span></p>
<p>The <strong>coefficient of determination</strong> <span class="math inline">\(R^2\)</span> <code>rsq</code> (where <span class="math inline">\(\bar{y}\)</span> is the mean of <span class="math inline">\(y\)</span>):</p>
<p><span class="math display">\[ 1- \frac{\sum \left( y_i - \hat{y}_i \right)^2}{\sum \left( y_i - \bar{y} \right)^2} \]</span></p>
<p>Good predictive models will have values of <code>rmse</code> and <code>mae</code> close to zero, and values of <code>rsq</code> close to one.</p>
<p>Let’s wrap the three metrics into a <code>metrics_regression</code> <code>metric_set</code>:</p>
<pre class="r"><code>metrics_regression &lt;- metric_set(rmse, mae, rsq)</code></pre>
</div>
<div id="selecting-a-model-with-cross-validation" class="section level2">
<h2>Selecting a model with cross validation</h2>
<p>We use <code>fit_resamples</code> to evaluate each workflow with the cross validation scheme defined in <code>bh_folds</code>. The function will return the values of <code>metrics_regression</code> averaged across all folds and repeats.</p>
<p>The outputs have <code>tibble</code> format, so I am storing them into variables and adding a column with the <code>model</code> description.</p>
<pre class="r"><code>set.seed(1212)
lm_fit &lt;- fit_resamples(bh_lm_wf, bh_folds, metrics = metrics_regression) %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;lm&quot;)

set.seed(1212)
rt_fit &lt;- fit_resamples(bh_rt_wf, bh_folds, metrics = metrics_regression) %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;rt&quot;)</code></pre>
<p>Let’s visualise the metrics for each model:</p>
<pre class="r"><code>bind_rows(lm_fit, rt_fit) %&gt;%
  select(.metric, mean, std_err, model) %&gt;%
  ggplot(aes(x = model, y = mean, ymin = mean - 1.96*std_err, ymax = mean + 1.96*std_err)) +
  geom_pointrange() + 
  theme_bw() +
  labs(y = &quot;confidence interval&quot;) +
  facet_grid(. ~ .metric)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>We observe that the regression tree model has slightly better metrics than linear regression. Let’s present each of them in tabular form:</p>
<pre class="r"><code>lm_fit %&gt;%
  select(model, .metric, mean, std_err)</code></pre>
<pre><code>## # A tibble: 3 x 4
##   model .metric  mean std_err
##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 lm    mae     3.70   0.0755
## 2 lm    rmse    5.13   0.166 
## 3 lm    rsq     0.709  0.0124</code></pre>
<pre class="r"><code>rt_fit %&gt;%
  select(model, .metric, mean, std_err)</code></pre>
<pre><code>## # A tibble: 3 x 4
##   model .metric  mean std_err
##   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;
## 1 rt    mae     3.16   0.0666
## 2 rt    rmse    4.63   0.180 
## 3 rt    rsq     0.761  0.0128</code></pre>
</div>
<div id="fitting-the-chosen-model-with-the-train-test" class="section level2">
<h2>Fitting the chosen model with the train test</h2>
<p>We have chosen regression trees to predict data. Let’s fit the model to the whole train set:</p>
<pre class="r"><code>fitted_model &lt;- bh_rt_wf %&gt;%
  fit(training(bh_split))</code></pre>
<p>And evaluate the metrics on the same train set:</p>
<pre class="r"><code>predict_train &lt;- fitted_model %&gt;%
  predict(training(bh_split)) %&gt;%
  bind_cols(training(bh_split)) %&gt;%
  mutate(sample = &quot;train&quot;)

predict_train %&gt;%
  metrics_regression(truth = medv, estimate = .pred)</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       4.00 
## 2 mae     standard       2.83 
## 3 rsq     standard       0.817</code></pre>
</div>
<div id="evaluating-performance-on-the-test-set" class="section level2">
<h2>Evaluating performance on the test set</h2>
<p>To check the performance on unseen data, we need to evaluate performance on the test set:</p>
<pre class="r"><code>predict_test &lt;- fitted_model %&gt;%
  predict(testing(bh_split)) %&gt;%
  bind_cols(testing(bh_split)) %&gt;%
  mutate(sample = &quot;test&quot;)

predict_test %&gt;%
  metrics_regression(truth = medv, estimate = .pred)</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       5.34 
## 2 mae     standard       3.46 
## 3 rsq     standard       0.632</code></pre>
<p>We observe that the metrics on the test set are worse than on the train set, so the chosen model has slight variance.</p>
<p>We can also visualize the <strong>predicted vs real</strong> value for each dataset. Ideally, most of the dots of this plot should be over the dashed line of intercept zero and slope one.</p>
<pre class="r"><code>bind_rows(predict_train, predict_test) %&gt;%
  ggplot(aes(medv, .pred)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, size = 0.3, linetype = &quot;dashed&quot;) +
  facet_grid(. ~ sample) +
  theme_bw()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>From the plot, we observe that the model has collapsed all possible values of <code>medv</code> into seven values, which are the same for the train and test sets. This is a result of the <a href="https://en.wikipedia.org/wiki/Recursive_partitioning">recursive partitioning</a> strategy adopted by regression tree models. Data are grouped into seven subsets, and all elements of each subset are predicted with the same value. As a result, some observations for each of the seven groups have a poor fit.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><code>BostonHousing</code>: Boston Housing Data <a href="https://rdrr.io/cran/mlbench/man/BostonHousing.html" class="uri">https://rdrr.io/cran/mlbench/man/BostonHousing.html</a></li>
<li>Harrison, D. and Rubinfeld, D.L. (1978). Hedonic prices and the demand for clean air. <em>Journal of Environmental Economics and Management</em>, 5, 81–102.</li>
<li>Therneau, T. M. and Atkinson, E. J. (2019). <em>An Introduction to Recursive Partitioning Using the RPART Routines</em>. Available at: <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf" class="uri">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a></li>
</ul>
<p><em>Built with R 4.0.3 and tidymodels 0.1.2</em></p>
</div>
