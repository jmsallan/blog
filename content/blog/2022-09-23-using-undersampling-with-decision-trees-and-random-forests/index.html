---
title: Using undersampling with decision trees and random forests
author: Jose M Sallan
date: '2022-09-23'
slug: using-undersampling-with-decision-trees-and-random-forests
categories:
  - R
tags:
  - decision trees
  - machine learning
  - random forests
  - R
  - tidymodels
meta_img: images/image.png
description: Description for the page
---



<p>In this post, I will present a classification job on an unbalanced dataset. In an unbalanced dataset, the target variable has an uneven distribution of observations. I will be using <code>tidymodels</code> for the prediction workflow, <code>BAdatasets</code> to retrieve the data and <code>themis</code> to perform undersampling.</p>
<pre class="r"><code>library(tidymodels)
library(themis)
library(BAdatasets)</code></pre>
<p>We will be using the LoanDefaults dataset. It is a dataset of defaults payments in Taiwan, presented in Yeh &amp; Lien (2009).</p>
<pre class="r"><code>data(&quot;LoanDefaults&quot;)</code></pre>
<p>When plotting the proportion of observations for each case, we observe that the number of negative cases (not defaulted) is much larger than the positive cases (defaulted).</p>
<pre class="r"><code>LoanDefaults %&gt;%
  ggplot(aes(factor(default))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels=percent) +
  labs(x = &quot;default&quot;, y = &quot;% of cases&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-3-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="setting-workflow-elements" class="section level2">
<h2>Setting workflow elements</h2>
<p>Let’s define the elements of the <code>tidymodels</code> workflow. We start transforming the target <code>default</code> variable into a factor, setting the level of positive cases first.</p>
<pre class="r"><code>LoanDefaults &lt;- LoanDefaults %&gt;%
  mutate(default = factor(default, levels = c(&quot;1&quot;, &quot;0&quot;)))</code></pre>
<p>Then, we need to split the data into train and test. As the dataset is large, I have selected a large <code>prop</code> value. Being the dataset unbalanced, it is important to set <code>strata = default</code>, so that the distribution of target variable in train and test sets will be similar to the original dataset. I have applied a similar logic to split the training set into four <code>folds</code> to apply cross validation.</p>
<pre class="r"><code>set.seed(1313)
split &lt;- initial_split(LoanDefaults, prop = 0.9, strata = &quot;default&quot;)

folds &lt;- vfold_cv(training(split), v = 10, strata = default)</code></pre>
<p>To evaluate model performance, I have chosen the following metrics:</p>
<ul>
<li><code>accuracy</code>: fraction of observations correctly classified.</li>
<li>sensitivity <code>sens</code>: the fraction of positive elements correctly classified.</li>
<li>specificity <code>spec</code>: the fraction of negative elements correctly classified.</li>
<li>area under the ROC curve <code>roc_auc</code>: a parameter assessing the tradeoff between <code>sens</code> and <code>spec</code>.</li>
</ul>
<pre class="r"><code>class_metrics &lt;- metric_set(accuracy, sens, spec, roc_auc)</code></pre>
<p>I will be using two models to train: a decision tree <code>dt</code> and a random forest <code>rf</code>. I will set standard values of parameter for each model. Hyperparameter tuning (not presented here) yields little influence of these parameters in model results.</p>
<pre class="r"><code>dt &lt;- decision_tree(mode = &quot;classification&quot;, cost_complexity = 0.1, min_n = 10) %&gt;%
  set_engine(&quot;rpart&quot;)

rf &lt;- rand_forest(mode = &quot;classification&quot;, trees = 50, mtry = 5, min_n = 10) %&gt;%
  set_engine(&quot;ranger&quot;)</code></pre>
<p>The preprocessing recipe is quite complex in this case:</p>
<ul>
<li>the variable <code>ID</code> is removed.</li>
<li>variables <code>PAY0</code> to <code>PAY6</code> are removed.</li>
<li>variables <code>payment_status</code>, <code>MARRIAGE</code> and <code>EDUCATION</code> are transformed so that abormal values are set to <code>NA</code>.</li>
<li><code>NA</code> values of predictors are imputed usign <em>k</em> nearest neighbors.</li>
<li><code>SEX</code>, <code>MARRIAGE</code> and <code>EDUCATION</code> are transformed into factors.</li>
</ul>
<pre class="r"><code>rec_unb &lt;- training(split) %&gt;%
  recipe(default ~ .) %&gt;%
  step_rm(ID) %&gt;%
  step_mutate(payment_status = ifelse(PAY_0 &lt; 1, 0, PAY_0)) %&gt;%
  step_rm(PAY_0:PAY_6) %&gt;%
  step_num2factor(SEX, levels = c(&quot;male&quot;, &quot;female&quot;)) %&gt;%
  step_mutate(MARRIAGE = ifelse(!MARRIAGE %in% 1:3, NA, MARRIAGE)) %&gt;%
  step_mutate(EDUCATION = ifelse(!EDUCATION %in% 1:4, NA, EDUCATION)) %&gt;%
  step_num2factor(MARRIAGE, levels = c(&quot;marriage&quot;, &quot;single&quot;, &quot;others&quot;)) %&gt;% 
  step_num2factor(EDUCATION, levels = c(&quot;graduate&quot;, &quot;university&quot;, &quot;high_school&quot;, &quot;others&quot;)) %&gt;%
  step_impute_knn(all_predictors(), neighbors = 3)</code></pre>
<p>Now we can test the two models using cross validation. We start with the decision tree:</p>
<pre class="r"><code>dt_cv_unb &lt;- fit_resamples(object = dt,
                       preprocessor = rec_unb,
                       resamples = folds,
                       metrics = class_metrics)

rf_cv_unb &lt;- fit_resamples(object = rf,
                       preprocessor = rec_unb,
                       resamples = folds,
                       metrics = class_metrics)</code></pre>
<p>The metrics for the decision tree are:</p>
<pre class="r"><code>dt_cv_unb %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 × 6
##   .metric  .estimator  mean     n  std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.820    10 0.00140  Preprocessor1_Model1
## 2 roc_auc  binary     0.644    10 0.00321  Preprocessor1_Model1
## 3 sens     binary     0.329    10 0.00661  Preprocessor1_Model1
## 4 spec     binary     0.959    10 0.000712 Preprocessor1_Model1</code></pre>
<p>And the metrics for cross validation:</p>
<pre class="r"><code>rf_cv_unb %&gt;%
  collect_metrics()</code></pre>
<pre><code>## # A tibble: 4 × 6
##   .metric  .estimator  mean     n std_err .config             
##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               
## 1 accuracy binary     0.818    10 0.00242 Preprocessor1_Model1
## 2 roc_auc  binary     0.759    10 0.00374 Preprocessor1_Model1
## 3 sens     binary     0.360    10 0.00680 Preprocessor1_Model1
## 4 spec     binary     0.948    10 0.00176 Preprocessor1_Model1</code></pre>
<p>The sensitivity of the two models is quite low. This is not good, because it means that a default will be undetected using these models.</p>
</div>
<div id="model-with-undersampling" class="section level2">
<h2>Model with undersampling</h2>
<p>A commonly used technique to improve classification of unbalanced data is to modify the training set so that it has the same number of positives and negatives. There are two ways of doing this:</p>
<ul>
<li><strong>oversampling:</strong> creating additional artificial observations for the less frequent case.</li>
<li><strong>undersampling:</strong> removing observations of the most frequent case.</li>
</ul>
<p>In tidymodels, those recipes are implemented with the <code>themis</code> package.</p>
<p>For this dataset, I have decided to undersample the training set using the <code>step_downsample</code>. The use of oversampling has lead to unsatisfactory results.</p>
<pre class="r"><code>rec_us &lt;- training(split) %&gt;%
  recipe(default ~ .) %&gt;%
  step_rm(ID) %&gt;%
  step_mutate(payment_status = ifelse(PAY_0 &lt; 1, 0, PAY_0)) %&gt;%
  step_rm(PAY_0:PAY_6) %&gt;%
  step_num2factor(SEX, levels = c(&quot;male&quot;, &quot;female&quot;)) %&gt;%
  step_mutate(MARRIAGE = ifelse(!MARRIAGE %in% 1:3, NA, MARRIAGE)) %&gt;%
  step_mutate(EDUCATION = ifelse(!EDUCATION %in% 1:4, NA, EDUCATION)) %&gt;%
  step_num2factor(MARRIAGE, levels = c(&quot;marriage&quot;, &quot;single&quot;, &quot;others&quot;)) %&gt;% 
  step_num2factor(EDUCATION, levels = c(&quot;graduate&quot;, &quot;university&quot;, &quot;high_school&quot;, &quot;others&quot;)) %&gt;%
  step_impute_knn(all_predictors(), neighbors = 3) %&gt;%
  step_downsample(default, under_ratio = 1)</code></pre>
<p>Let’s examine the distribution of the target variable after downsampling:</p>
<pre class="r"><code>rec_us %&gt;%
  prep() %&gt;%
  juice() %&gt;%
    ggplot(aes(factor(default))) + 
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels=percent) +
  labs(x = &quot;default&quot;, y = &quot;% of cases&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We observe that all the dataset for which is trained the model is balanced. Let’s proceed to train the models with the undersampled recipe:</p>
<pre class="r"><code>dt_cv_us &lt;- fit_resamples(object = dt,
                       preprocessor = rec_us,
                       resamples = folds,
                       metrics = class_metrics)

rf_cv_us &lt;- fit_resamples(object = rf,
                       preprocessor = rec_us,
                       resamples = folds,
                       metrics = class_metrics)</code></pre>
<p>As the metrics are presented in data frames, we can put them all together and examine the performance of the four models at once. As variability of metrics can be an issue when balancing data, I have added errorbars for each parameter.</p>
<pre class="r"><code>t1 &lt;- dt_cv_unb %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;tree&quot;, train = &quot;unbalanced&quot;)

t2 &lt;- rf_cv_unb %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;forest&quot;, train = &quot;unbalanced&quot;)

t3 &lt;- dt_cv_us %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;tree&quot;, train = &quot;balanced&quot;)

t4 &lt;- rf_cv_us %&gt;%
  collect_metrics() %&gt;%
  mutate(model = &quot;forest&quot;, train = &quot;balanced&quot;)

bind_rows(t1, t2, t3, t4) %&gt;%
  ggplot(aes(x =.metric, y = mean, ymin = mean - std_err, ymax = mean + std_err, fill = train)) +
  geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) +
  geom_errorbar(position=position_dodge(width=1), width = 0.5) +
  scale_fill_manual(values = c(&quot;#990000&quot;, &quot;#FF8000&quot;)) +
  facet_grid(. ~ model) +
  labs(x = &quot;metric&quot;, y = &quot;value&quot;)</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>We can observe that undersampling increases values of sensitivity at the cost of reducing specificity. Undersampling leads to slightly worse values of accuracy, and slightly better values of AUC.</p>
</div>
<div id="prediction-for-the-undersampled-model" class="section level2">
<h2>Prediction for the undersampled model</h2>
<p>Let’s choose the undersampled model trained with the random forest model. We can test this model on both the test and train set.</p>
<p>We start defining the model using a workflow:</p>
<pre class="r"><code>rf_us &lt;- workflow() %&gt;%
  add_recipe(rec_us) %&gt;%
  add_model(rf) %&gt;%
  fit(training(split))</code></pre>
<p>Let’s obtain the predicted values for train and test sets. Note that the prediction on the train set is with the original data, not with the undersampled ones. Undersampling only takes place to fit the model.</p>
<pre class="r"><code>pred_train &lt;- rf_us %&gt;%
  predict(training(split)) %&gt;%
  bind_cols(training(split) %&gt;% select(default))

pred_test &lt;- rf_us %&gt;%
  predict(testing(split)) %&gt;%
  bind_cols(testing(split) %&gt;% select(default))</code></pre>
<p>The confusion matrices show that the model tends to inflate false positives to obtain a decent value of specifity:</p>
<pre class="r"><code>class_metrics2 &lt;- metric_set(accuracy, sens, spec)

pred_train %&gt;%
  conf_mat(truth = default, estimate = .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction     1     0
##          1  5875  3931
##          0    97 17096</code></pre>
<pre class="r"><code>pred_test %&gt;%
  conf_mat(truth = default, estimate = .pred_class)</code></pre>
<pre><code>##           Truth
## Prediction    1    0
##          1  424  577
##          0  240 1760</code></pre>
<p>Let’s compare accuracy, sensitivity and specificity for train and test sets:</p>
<pre class="r"><code>class_metrics2 &lt;- metric_set(accuracy, sens, spec)

t_train &lt;- pred_train %&gt;%
  class_metrics2(truth = default, estimate = .pred_class) %&gt;%
  mutate(set = &quot;train&quot;)
t_test &lt;- pred_test %&gt;%
  class_metrics2(truth = default, estimate = .pred_class) %&gt;% 
  mutate(set = &quot;test&quot;)

bind_rows(t_train, t_test) %&gt;%
  ggplot(aes(.metric, .estimate, fill = set)) +
  geom_bar(stat = &quot;identity&quot;, position = &quot;dodge&quot;) +
  scale_fill_manual(values = c(&quot;#990000&quot;, &quot;#FF8000&quot;)) +
  labs(x = &quot;metric&quot;, y = &quot;value&quot;) +
  theme_minimal()</code></pre>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We observe that undersampling has improved sensitivity at the price of worsening accuracy and specificity. In this case, undersampling has somewhat improved our model, but this is not always the case. Specially when we use oversampling, where we introduce artificial observations to train the model. Some alternative approaches to deal with unbalanced datasets can be found at the StackExchange question cited below.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="https://github.com/jmsallan/BAdatasets" class="uri">https://github.com/jmsallan/BAdatasets</a>
* <code>themis</code> package website: <a href="https://themis.tidymodels.org/" class="uri">https://themis.tidymodels.org/</a>
* StackExchange question: <em>Are unbalanced datasets problematic, and (how) does oversampling (purport to) help?</em> <a href="https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he" class="uri">https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he</a>
* Yeh, I. C., &amp; Lien, C. H. (2009). The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients. <em>Expert Systems with Applications</em>, 36(2), 2473-2480. <a href="https://doi.org/10.1016/j.eswa.2007.12.020" class="uri">https://doi.org/10.1016/j.eswa.2007.12.020</a></p>
</div>
<div id="session-info" class="section level2">
<h2>Session info</h2>
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>## R version 4.2.1 (2022-06-23)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Linux Mint 19.2
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3
## LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so
## 
## locale:
##  [1] LC_CTYPE=es_ES.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=es_ES.UTF-8        LC_COLLATE=es_ES.UTF-8    
##  [5] LC_MONETARY=es_ES.UTF-8    LC_MESSAGES=es_ES.UTF-8   
##  [7] LC_PAPER=es_ES.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=es_ES.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
##  [1] ranger_0.13.1      rpart_4.1.16       BAdatasets_0.1.0   themis_0.2.1      
##  [5] yardstick_0.0.9    workflowsets_0.2.1 workflows_0.2.6    tune_0.2.0        
##  [9] tidyr_1.2.0        tibble_3.1.6       rsample_0.1.1      recipes_0.2.0     
## [13] purrr_0.3.4        parsnip_0.2.1      modeldata_0.1.1    infer_1.0.0       
## [17] ggplot2_3.3.5      dplyr_1.0.9        dials_0.1.1        scales_1.2.0      
## [21] broom_0.8.0        tidymodels_0.2.0  
## 
## loaded via a namespace (and not attached):
##  [1] lubridate_1.8.0    doParallel_1.0.17  DiceDesign_1.9     tools_4.2.1       
##  [5] backports_1.4.1    bslib_0.3.1        utf8_1.2.2         R6_2.5.1          
##  [9] DBI_1.1.2          colorspace_2.0-3   nnet_7.3-17        withr_2.5.0       
## [13] tidyselect_1.1.2   parallelMap_1.5.1  compiler_4.2.1     cli_3.3.0         
## [17] labeling_0.4.2     bookdown_0.26      sass_0.4.1         checkmate_2.1.0   
## [21] stringr_1.4.0      digest_0.6.29      rmarkdown_2.14     unbalanced_2.0    
## [25] pkgconfig_2.0.3    htmltools_0.5.2    parallelly_1.31.1  lhs_1.1.5         
## [29] highr_0.9          fastmap_1.1.0      rlang_1.0.2        rstudioapi_0.13   
## [33] BBmisc_1.12        FNN_1.1.3          farver_2.1.0       jquerylib_0.1.4   
## [37] generics_0.1.2     jsonlite_1.8.0     magrittr_2.0.3     ROSE_0.0-4        
## [41] Matrix_1.5-1       Rcpp_1.0.8.3       munsell_0.5.0      fansi_1.0.3       
## [45] GPfit_1.0-8        lifecycle_1.0.1    furrr_0.3.0        stringi_1.7.6     
## [49] pROC_1.18.0        yaml_2.3.5         MASS_7.3-58        plyr_1.8.7        
## [53] grid_4.2.1         parallel_4.2.1     listenv_0.8.0      crayon_1.5.1      
## [57] lattice_0.20-45    splines_4.2.1      mlr_2.19.0         knitr_1.39        
## [61] pillar_1.7.0       future.apply_1.9.0 codetools_0.2-18   fastmatch_1.1-3   
## [65] glue_1.6.2         evaluate_0.15      ParamHelpers_1.14  blogdown_1.9      
## [69] data.table_1.14.2  vctrs_0.4.1        foreach_1.5.2      RANN_2.6.1        
## [73] gtable_0.3.0       future_1.25.0      assertthat_0.2.1   xfun_0.30         
## [77] gower_1.0.0        prodlim_2019.11.13 class_7.3-20       survival_3.4-0    
## [81] timeDate_3043.102  iterators_1.0.14   hardhat_0.2.0      lava_1.6.10       
## [85] globals_0.14.0     ellipsis_0.3.2     ipred_0.9-12</code></pre>
</div>
